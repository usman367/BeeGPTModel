{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f056d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ef47c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '17flowers'\n",
    "image_size = (128, 128) # Images will be resized to 128 by 128\n",
    "batch_size = 40\n",
    "num_classes = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING AND PREPROCESSING\n",
    "# Load the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "class_names = sorted(os.listdir(os.path.join(data_dir))) # Read the class names\n",
    "# print(\"Class name: \", class_names)\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(data_dir, class_name) # Path to directory\n",
    "    for img_file in os.listdir(class_dir):\n",
    "        img_path = os.path.join(class_dir, img_file) # Path to the image\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size) # Load and resize the image\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img) # Convert the image to an array\n",
    "        \n",
    "        # Append the image and the class label to the respective lists\n",
    "        data.append(img)\n",
    "        labels.append(class_name)\n",
    "\n",
    "        \n",
    "data = np.array(data)\n",
    "# print(len(data))\n",
    "\n",
    "labels = np.array(labels)\n",
    "# print(\"Labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ded6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA CONVERSION AND SPLITTING\n",
    "### Convert class names to integer labels\n",
    "# Create a mapping from class name to integer labels\n",
    "label_to_index = {label: i for i, label in enumerate(class_names)}\n",
    "print(\"label_to_index: \", label_to_index)\n",
    "\n",
    "# Converts class names to integer labels\n",
    "labels = np.array([label_to_index[label] for label in labels])\n",
    "print(\"labels: \", labels)\n",
    "print(\"labels length: \", len(labels))\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "# 70% will be used for training\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "print(\"x_train size: \", len(x_train))\n",
    "print(\"x_temp size: \", len(x_temp))\n",
    "\n",
    "\n",
    "# SPlit the data from above validations and testing (15% each now)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(\"x_val size: \", len(x_val))\n",
    "print(\"y_val size: \", len(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer labels to one-hot encoded labels\n",
    "# To respresent class labels as categorical vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"y_train: \", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator for the dataset\n",
    "train_iterator = iter(x_train)\n",
    "\n",
    "# Get the first batch\n",
    "first_batch = next(train_iterator)\n",
    "\n",
    "batch_shape = first_batch[0].shape\n",
    "print(\"Shape of batch:\", batch_shape)\n",
    "\n",
    "# Get the size (dimensions) of the first image\n",
    "image_height, image_width, num_channels = first_batch.shape\n",
    "print(\"Image size (height x width x channels):\", image_height, \"x\", image_width, \"x\", num_channels)\n",
    "\n",
    "# Check the size of the first batch\n",
    "batch_height = len(first_batch[0])  # Assuming the first element of the batch is the images\n",
    "print(\"Height of the images in the first batch:\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6278846",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA NORMALIZATION\n",
    "# The RGB channel values are in the [0, 255] range.\n",
    "# This is not ideal for a neural network\n",
    "# Normalize the pixel values to [0, 1]\n",
    "\n",
    "# print (x_train)\n",
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "x_test = x_test / 255.0\n",
    "# print (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with regularization\n",
    "inputs = Input(shape=(image_size[0], image_size[1], 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D((4, 4))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((4, 4))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((4, 4))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104aff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using Adam optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c4b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 25\n",
    "hist = model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc15d9",
   "metadata": {},
   "source": [
    "# Plot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise our loss\n",
    "# Loss: measure of how well a model's predictions match the true target values in the training dataset. \n",
    "# The goal of training a machine learning model is to minimize the loss function, as this indicates that the model is making more accurate predictions.\n",
    "# When training a model, it starts with some initial set of parameters (weights and biases) and makes predictions on the training data. \n",
    "# The loss function then quantifies the difference between these predictions and the actual target values. \n",
    "# The larger the difference, the higher the loss, indicating that the model is performing poorly.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss') # Training Loss\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss') # Validation Loss\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Our validation loss is increasing so the model is overfitting here (as mentioned in the video)\n",
    "# Might need to perform some regularization as model is overfitting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ffa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save(os.path.join('models\\\\AllMpsIncreadToFour', 'AllMPsIncreasedFour13.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7a524",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "eb83c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "59475324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 232ms/step\n"
     ]
    }
   ],
   "source": [
    "new_image_paths = ['C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Bluebell.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Buttercup.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Coltsfoot.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Cowslip.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Crocus.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Daffodil.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Daisy.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Dandelion.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Fritillary.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Iris.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\LilValley.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Pansy.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Snowdrop.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Sunflower.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Tigerly.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Tulip.jpg', 'C:\\\\Users\\\\usman\\\\ML\\\\BeeGPTModel\\\\TestPics\\\\Windflower.jpg']\n",
    "new_images = []  # Store preprocessed images here\n",
    "for image_path in new_image_paths:\n",
    "    new_image = load_img(image_path, target_size=image_size)\n",
    "    new_image = img_to_array(new_image)\n",
    "    new_image = new_image / 255.0\n",
    "    new_images.append(new_image)\n",
    "\n",
    "new_images = np.array(new_images)\n",
    "\n",
    "# Make predictions for all new images\n",
    "predictions = model.predict(new_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c7db65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "class_labels = list(label_to_index.keys())  # Get class labels from the label-to-index mapping\n",
    "predicted_classes = [class_labels[i] for i in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d28cb95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bluebell', 'Buttercup', 'Tulip', 'Cowslip', 'Bluebell', 'Cowslip', 'Crocus', 'Fritillary', 'Fritillary', 'Bluebell', 'Snowdrop', 'Crocus', 'Snowdrop', 'Cowslip', 'Tigerlily', 'Fritillary', 'Fritillary']\n"
     ]
    }
   ],
   "source": [
    "print(predicted_classes) ## Can't recognise tulips???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ed392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
